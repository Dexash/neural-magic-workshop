= Neural Magic Overview

Neural Magic, established in 2018 by MIT professor Nir Shavit and research scientist Alex Matveev, is a Series-A company specializing in AI model optimization and accelerated inference serving. 
Headquartered in Somerville, MA, the company focuses on enabling enterprise deployment of open-source machine learning models across edge, datacenter, and cloud environments.

Neural Magic is a pioneering AI software company focused on optimizing and accelerating machine learning models for efficient deployment across various computing environments, including cloud, data centers, and edge devices. 
Their core innovations involve *sparsification*, *quantization*, and *efficient inference serving*, allowing AI models to run smoothly on standard CPUs and GPUs without sacrificing performance.

== Technical Capabilities

=== Sparsification: Optimizing AI by Removing Redundant Parameters

Sparsification is a technique that removes unnecessary weights from AI models, reducing model size while maintaining accuracy. 
This allows AI models to run efficiently on CPUs, eliminating the need for expensive GPUs.

* Reduces unnecessary parameters in AI models.
* Increases inference speed while maintaining accuracy.
* Lowers computational cost for AI workloads.

link:https://neuralmagic.com/sparsification/[Learn more]

=== Quantization: Reducing Model Size While Maintaining Accuracy

Quantization converts high-precision model parameters into lower-precision representations, making models smaller and more efficient. 
This enables faster inference on commodity hardware while keeping accuracy loss minimal.

* Compresses AI models by lowering numerical precision.
* Enables faster execution on general-purpose CPUs.
* Reduces storage and memory footprint.

link:https://neuralmagic.com/quantization/[Learn more]

=== vLLM: Efficient Large Language Model Processing

vLLM is an optimized inference framework that improves the efficiency of large language models by managing memory more effectively. 
It enables seamless model deployment while ensuring fast and efficient querying.

* Reduces memory overhead and latency in LLM inference.
* Enables parallel query handling and optimized memory management.
* Supports various hardware platforms, including NVIDIA, AMD, Intel GPUs, AWS Inferentia, and Google TPUs.

link:https://vllm.ai/[Learn more] +
link:https://neuralmagic.com/nm-vllm/[Learn more]

=== DeepSparse: High-Performance AI Inference on CPUs

DeepSparse is Neural Magic's inference engine that maximizes CPU efficiency for AI workloads. 
It leverages sparsification and quantization to accelerate model inference without specialized hardware.

* AI inference engine leveraging sparsification for efficient execution.
* Optimized for *computer vision (CV), natural language processing (NLP), and large language models (LLMs).*
* Seamlessly integrates with *Red Hat OpenShift AI*.

link:https://neuralmagic.com/deepsparse/[Learn more]

== Neural Magic and Red Hat AI

Neural Magic's AI optimizations complement *Red Hat OpenShift AI* and *RHEL AI*, enabling:

* *Optimized LLM deployments* across hybrid cloud environments.
* *Cost-effective AI inferencing* without expensive GPUs.
* *Seamless integration with containerized environments* for scalable AI workloads.

== Additional Resources

* link:https://neuralmagic.com/[Neural Magic Homepage]
* link:https://neuralmagic.com/blog/[Neural Magic Blog]
* link:https://www.redhat.com/en/solutions/ai-ml[Red Hat AI & ML]
* link:https://github.com/neuralmagic/deepsparse[DeepSparse GitHub]
* link:https://github.com/vllm-project/vllm[vLLM GitHub]

