= {lab_name}
include::_attributes.adoc[]

== Lab Overview

This lab will illustrate how to leverage Neural Magic's model optimization techniques (llm-compressor) on OpenShift AI to enable fitting your model to your hardware.
You will learn how to achieve significant performance improvements and cost reductions through model sparsification and quantization.

The information, code, pipelines and techniques it contains are illustrations of what a first prototype could look like.

== Disclaimer

This lab is an example of what a customer could use to optimize its models for enhanced inferencing while reducing hardware costs using OpenShift AI.

This lab makes use of "small" size large language models (LLM) for speeding up the process, but the same techniques would apply for larger models.

== Repository

This repository link:https://github.com/luis5tb/neural-magic-workshop/[window=_blank] contains workshop materials and examples for creating optimized models with Neural Magic solutions on OpenShift.
This workshop is part of the OpenShift Commons initiative.

=== Repository Structure

==== Lab Content

The lab content is at the `content` folder, on the `modules/ROOT/pages` directory.

==== Lab Material

The `lab-materials/` directory contains the different materials for the different points:

* Section 2 (folder `02`):
** Sample pipeline yaml file: `test_pipeline.yaml`
* Section 3 (folder `03`):
** Jupyter notebooks for model optimization techniques and evaluation:
*** Weights quantization: `int4`
*** Weights and activations quantization: `fp8` and `int8`
* Section 4 (folder `04`):
** Python script for testing deployed models: `request.py`
* Section 5 (folder `05`):
** Pipeline source file (`quantization_pipeline.py`) for optimizing models while maintaining accuracy, also includes evaluation steps.

=== Prerequisites

* OpenShift AI
* NVIDIA GPUs

== Workshop Timetable

This is a tentative timetable for the materials that will be presented today.

[width="90%",cols="3,^2,^2,10",options="header"]
|=========================================================
| Time | Section | Type | Description

|0 - 15 min |Intro Talk & AI Optimization Overview |Presentation
a|- What is OpenShift AI? (Portfolio overview)
- Why Optimize AI Models? (Challenges with GPUs, benefits of compression)
- Introduction to vLLM & Neural Magic

|15 - 25 min |Workshop Setup |Hands-On
a|- Guide participants through demo.redhat.com setup
- Explain workshop environment & MinIO storage

|25 - 45 min |Workbench Creation & Model Quantization |Hands-On
a|- Hands-on: Step-by-step quantization of a model manually
- Int4
- Int8
- fp8

|45 - 65 min |Deploy Base and Optimized Model |Hands-On
a|- Deploy a base AI model using OpenShift AI ServingRuntime
- Deploy a pre-optimized model from MinIO
- Compare performance vs. base model

|65 - 85 min |Pipeline Creation & Deployment |Hands-On
a|- Create an AI pipeline backed by MinIO
- Import a pipeline YAML and trigger the pipeline with parameters

|85 - 90 min |Q&A + Wrap-Up |Hands-On
a|- Recap learnings
- Open discussion on real-world AI optimization

|=========================================================


== Contributing

If you are interested in contributing to this project, consult this GitHub Repo: link:https://github.com/luis5tb/neural-magic-workshop/[window=_blank]
